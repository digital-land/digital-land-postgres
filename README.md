# Manage digital land postgres

**[WIP] Don't expect this to fully, reliably or consistently work. It's early days**

This is a temporary repo to assist in loading data into the digital last postgres db. When 
we have a fully automated process this can be archived.

To use this, create a temporary ec2 instances with ssh access, but limit access from your current IP. 

Use that instance to download scripts in the runner directory and use them to load data.


## How to use this thing locally

Prerequisites

A running postgres server - tested with PostgreSQL 13


Get an up to date version of the entity database generated by the pipeline

    curl -O https://digital-land-collection.s3.eu-west-2.amazonaws.com/entity.sqlite3

Export from sqlite db to csv using sqlite command line.

    sqlite3 entity.sqlite3 ".read export.sql"

Build and start the postgis docker image

    docker-compose build

    docker-compose up -d

Load csv file exported form entity.sqlite3 into a running postgres server on localhost

    docker-compose build

    docker-compose run --rm runner ./fetch_and_load_data.sh

